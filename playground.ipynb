{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('eth-dl-rewards/code_preference_data', split='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5342f94465894518a36501006bc1028b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL = 'internlm/internlm2-7b-reward'\n",
    "model = AutoModel.from_pretrained(MODEL, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(prompt, solution):\n",
    "  messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": solution}\n",
    "  ]\n",
    "  encoded = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "  inputs = tokenizer(encoded, return_tensors='pt', truncation=True, max_length=4096).to('cuda')\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    reward = outputs.logits[0][0].item()\n",
    "    del inputs, outputs\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 101/4180 [00:15<12:12,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 94/100 (94.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|▍         | 200/4180 [00:29<12:46,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 194/200 (97.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|▋         | 300/4180 [00:53<16:22,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 289/300 (96.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|▉         | 400/4180 [01:14<14:19,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 385/400 (96.25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 500/4180 [01:40<10:27,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 483/500 (96.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|█▍        | 601/4180 [01:59<11:54,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 582/600 (97.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 700/4180 [02:17<13:54,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 681/700 (97.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|█▉        | 800/4180 [02:35<09:20,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 777/800 (97.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|██▏       | 900/4180 [02:51<14:07,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 875/900 (97.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|██▍       | 1000/4180 [03:07<09:22,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 973/1000 (97.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|██▋       | 1100/4180 [03:29<07:42,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1071/1100 (97.36%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|██▊       | 1201/4180 [03:52<11:30,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1168/1200 (97.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|███       | 1300/4180 [04:14<14:29,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1267/1300 (97.46%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  34%|███▎      | 1401/4180 [04:35<09:11,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1365/1400 (97.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|███▌      | 1500/4180 [04:53<07:49,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1460/1500 (97.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 1600/4180 [05:13<09:41,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1558/1600 (97.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|████      | 1702/4180 [05:31<05:12,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1654/1700 (97.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|████▎     | 1800/4180 [05:51<18:53,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1752/1800 (97.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|████▌     | 1900/4180 [06:09<09:32,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1847/1900 (97.21%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|████▊     | 2002/4180 [06:27<04:48,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1944/2000 (97.20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 2101/4180 [06:49<05:52,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2040/2100 (97.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  53%|█████▎    | 2201/4180 [07:04<05:03,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2136/2200 (97.09%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  55%|█████▌    | 2301/4180 [07:23<04:25,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2236/2300 (97.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  57%|█████▋    | 2400/4180 [07:39<03:48,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2331/2400 (97.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|█████▉    | 2502/4180 [07:57<03:56,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2428/2500 (97.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|██████▏   | 2600/4180 [08:12<06:43,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2525/2600 (97.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|██████▍   | 2701/4180 [08:37<07:16,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2621/2700 (97.07%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|██████▋   | 2801/4180 [08:58<06:38,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2720/2800 (97.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|██████▉   | 2900/4180 [09:18<06:52,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2815/2900 (97.07%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|███████▏  | 3001/4180 [09:37<03:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 2908/3000 (96.93%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|███████▍  | 3101/4180 [09:54<02:28,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3002/3100 (96.84%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 3201/4180 [10:12<02:55,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3098/3200 (96.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  79%|███████▉  | 3301/4180 [10:36<02:12,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3193/3300 (96.76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|████████▏ | 3400/4180 [10:59<02:04,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3290/3400 (96.76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  84%|████████▍ | 3501/4180 [11:17<02:05,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3385/3500 (96.71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  86%|████████▌ | 3601/4180 [11:35<02:23,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3481/3600 (96.69%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%|████████▊ | 3700/4180 [11:51<01:31,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3578/3700 (96.70%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|█████████ | 3800/4180 [12:11<01:08,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3673/3800 (96.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|█████████▎| 3901/4180 [12:28<00:32,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3768/3900 (96.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|█████████▌| 4000/4180 [12:49<00:44,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3868/4000 (96.70%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|█████████▊| 4100/4180 [13:11<00:11,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 3966/4100 (96.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4180/4180 [13:24<00:00,  5.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "correct = 0\n",
    "total = 0\n",
    "gaps = []\n",
    "for problem, accepted, rejected in tqdm(zip(ds['problem'], ds['accepted'], ds['rejected']), total=len(ds), desc='Evaluating'):\n",
    "  reward_accepted = get_reward(problem, accepted)\n",
    "  reward_rejected = get_reward(problem, rejected)\n",
    "  gaps.append(reward_accepted - reward_rejected)\n",
    "  if reward_accepted > reward_rejected:\n",
    "    correct += 1\n",
    "  total += 1\n",
    "\n",
    "  if total % 100 == 0:\n",
    "    print(f\"Correct: {correct}/{total} ({correct/total*100:.2f}%)\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 4062/4180 (97.18%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct: {correct}/{total} ({correct/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ds should be a list of tuples in the form [(problem, accepted, rejected), ...]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 55\u001b[0m precomputed_rewards \u001b[38;5;241m=\u001b[39m precompute_rewards(\u001b[43mds\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     56\u001b[0m correct, total, gaps \u001b[38;5;241m=\u001b[39m evaluate_from_precomputed(precomputed_rewards)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Results: Correct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def get_rewards_batch(batch):\n",
    "    prompts, solutions = zip(*batch)\n",
    "    messages = [\n",
    "        [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": solution}]\n",
    "        for prompt, solution in zip(prompts, solutions)\n",
    "    ]\n",
    "    encoded = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(encoded, return_tensors='pt', truncation=True, max_length=4096, padding=True).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        rewards = outputs.logits[:, 0].tolist()\n",
    "        del inputs, outputs\n",
    "        return rewards\n",
    "\n",
    "def precompute_rewards(ds, batch_size=10):\n",
    "    rewards = []\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc='Precomputing Rewards'):\n",
    "        batch = list(zip(ds['problem'][i:i + batch_size], ds['accepted'][i:i + batch_size], ds['rejected'][i:i + batch_size]))\n",
    "        accepted_batch = [(problem, accepted) for problem, accepted, _ in batch]\n",
    "        rejected_batch = [(problem, rejected) for problem, _, rejected in batch]\n",
    "\n",
    "        reward_accepted = get_rewards_batch(accepted_batch)\n",
    "        reward_rejected = get_rewards_batch(rejected_batch)\n",
    "\n",
    "        batch_rewards = list(zip(reward_accepted, reward_rejected))\n",
    "        rewards.extend(batch_rewards)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def evaluate_from_precomputed(rewards):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    gaps = []\n",
    "\n",
    "    for reward_accepted, reward_rejected in rewards:\n",
    "        gap = reward_accepted - reward_rejected\n",
    "        gaps.append(gap)\n",
    "        if reward_accepted > reward_rejected:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct, total, gaps\n",
    "\n",
    "# Example usage:\n",
    "# ds should be a list of tuples in the form [(problem, accepted, rejected), ...]\n",
    "batch_size = 8\n",
    "precomputed_rewards = precompute_rewards(ds, batch_size=batch_size)\n",
    "correct, total, gaps = evaluate_from_precomputed(precomputed_rewards)\n",
    "print(f\"Final Results: Correct: {correct}/{total} ({correct/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
