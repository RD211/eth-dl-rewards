{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('eth-dl-rewards/pref-data-math', split='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL = 'eth-dl-rewards/internlm2-7b-reward-math-30k'\n",
    "model = AutoModel.from_pretrained(MODEL, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(prompt, solution):\n",
    "  messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": solution}\n",
    "  ]\n",
    "  encoded = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "  with torch.no_grad():\n",
    "    inputs = tokenizer(encoded, return_tensors='pt', truncation=True, max_length=3500).to('cuda')\n",
    "    print(inputs['input_ids'].shape)\n",
    "    outputs = model(**inputs)\n",
    "    reward = outputs.logits[0][0].item()\n",
    "    del inputs\n",
    "    del outputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "correct = 0\n",
    "total = 0\n",
    "gaps = []\n",
    "model.eval()\n",
    "for problem, accepted, rejected in tqdm(zip(ds['problem'], ds['accepted'], ds['rejected']), total=len(ds), desc='Evaluating'):\n",
    "  reward_accepted = get_reward(problem, accepted)\n",
    "  reward_rejected = get_reward(problem, rejected)\n",
    "  gaps.append(reward_accepted - reward_rejected)\n",
    "  if reward_accepted > reward_rejected:\n",
    "    correct += 1\n",
    "  total += 1\n",
    "\n",
    "  if total % 100 == 0:\n",
    "    print(f\"Correct: {correct}/{total} ({correct/total*100:.2f}%)\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correct: {correct}/{total} ({correct/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def get_rewards_batch(batch):\n",
    "    prompts, solutions = zip(*batch)\n",
    "    messages = [\n",
    "        [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": solution}]\n",
    "        for prompt, solution in zip(prompts, solutions)\n",
    "    ]\n",
    "    encoded = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(encoded, return_tensors='pt', truncation=True, max_length=4096, padding=True).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        rewards = outputs.logits[:, 0].tolist()\n",
    "        del inputs, outputs\n",
    "        return rewards\n",
    "\n",
    "def precompute_rewards(ds, batch_size=10):\n",
    "    rewards = []\n",
    "    for i in tqdm(range(0, len(ds), batch_size), desc='Precomputing Rewards'):\n",
    "        batch = list(zip(ds['problem'][i:i + batch_size], ds['accepted'][i:i + batch_size], ds['rejected'][i:i + batch_size]))\n",
    "        accepted_batch = [(problem, accepted) for problem, accepted, _ in batch]\n",
    "        rejected_batch = [(problem, rejected) for problem, _, rejected in batch]\n",
    "\n",
    "        reward_accepted = get_rewards_batch(accepted_batch)\n",
    "        reward_rejected = get_rewards_batch(rejected_batch)\n",
    "\n",
    "        batch_rewards = list(zip(reward_accepted, reward_rejected))\n",
    "        rewards.extend(batch_rewards)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def evaluate_from_precomputed(rewards):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    gaps = []\n",
    "\n",
    "    for reward_accepted, reward_rejected in rewards:\n",
    "        gap = reward_accepted - reward_rejected\n",
    "        gaps.append(gap)\n",
    "        if reward_accepted > reward_rejected:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct, total, gaps\n",
    "\n",
    "# Example usage:\n",
    "# ds should be a list of tuples in the form [(problem, accepted, rejected), ...]\n",
    "batch_size = 8\n",
    "precomputed_rewards = precompute_rewards(ds, batch_size=batch_size)\n",
    "correct, total, gaps = evaluate_from_precomputed(precomputed_rewards)\n",
    "print(f\"Final Results: Correct: {correct}/{total} ({correct/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "config = AutoConfig.from_pretrained(\"internlm/internlm2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.auto_map['AutoModel'] = 'internlm/internlm2-7b--modeling_internlm2.InternLM2ForSequenceClassification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89eda8eb1a7b4c699af0ae7f31dc4774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626c0e46f8e4e30999b8769c587a7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/39.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6257be43572d483382be05bebedd2d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/69363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('eth-dl-rewards/pref-data-code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take first 60k examples for train and rest for eval\n",
    "train_ds = ds['train'].select(range(60000))\n",
    "eval_ds = ds['train'].select(range(60000, ds['train'].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['problem', 'accepted', 'rejected'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['problem', 'accepted', 'rejected'],\n",
       "        num_rows: 9363\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds = {\n",
    "    'train': train_ds,\n",
    "    'eval': eval_ds\n",
    "}\n",
    "from datasets import DatasetDict\n",
    "new_ds = DatasetDict(new_ds)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee46aa8e04e4e768fa05137a09c58f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0b7d8af0e747869c0361a1f50cd151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/60 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69797fcb7f8486e8ce25a60a49d2922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7592a53e36417b96314de6ccdcdd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/eth-dl-rewards/pref-data-code/commit/9619a3edd91cea6b93e96024c3da75ffb374111c', commit_message='Upload dataset', commit_description='', oid='9619a3edd91cea6b93e96024c3da75ffb374111c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/eth-dl-rewards/pref-data-code', endpoint='https://huggingface.co', repo_type='dataset', repo_id='eth-dl-rewards/pref-data-code'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds.push_to_hub('eth-dl-rewards/pref-data-code')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
