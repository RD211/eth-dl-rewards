model:
  model_name_or_path: "eth-dl-rewards/llama-3.1-8b-sft-math"
  max_length: 4096
  already_reward_model: true
  lora:
    enable: true
    rank: 32
    alpha: 64
    target_modules: "all-linear"
    dropout: 0.01
    bias: "none"

reward_model:
  model_name_or_path: "eth-dl-rewards/llama-3.1-8b-rm-math"

sampling_params:
  temperature: 1.0
  top_k: 50
  top_p: 1.0

dataset:
  datasets: 
    - name_or_path: "AI-MO/NuminaMath-CoT"
      split: "train"
      ratio: 1.0
      skip: 200000
  max_examples: 100000

huggingface:
  name: "eth-dl-rewards/llama-3.1-8b-ppo-math"
  push_to_hub: true

logging:
  wandb: true
  wandb_project: "train-ppo"
  wandb_run_name: "llama-3.1-8b-ppo-math"
  wandb_entity: "eth-dl-rewards"
  run_group: "math"
  wandb_tags: ["math", "ppo"]
  save_dir: "output"

train:
  gradient_accumulation_steps: 4
  gradient_checkpointing: false
  per_device_train_batch_size: 4
  lr_scheduler_type: "cosine"
  optimizer: "adamw_hf"
  epochs: 1
  max_steps: -1
  deepspeed_config_path: null


seed: 42
